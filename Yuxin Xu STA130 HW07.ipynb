{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0519304e",
   "metadata": {},
   "source": [
    "question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd8b51",
   "metadata": {},
   "source": [
    "1.Difference between Simple and Multiple Linear Regression & Benefits of the Latter: Simple Linear Regression (SLR) uses one predictor variable to predict an outcome, whereas Multiple Linear Regression (MLR) uses multiple predictor variables. The advantage of MLR is that it captures the effect of multiple factors on the outcome, providing a more accurate and comprehensive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9c88d",
   "metadata": {},
   "source": [
    "2.Continuous vs. Indicator Variables in Simple Linear Regression: A continuous variable has a full range of values (e.g., weight or temperature), while an indicator variable (binary) signals the presence or absence of a condition (e.g., male/female coded as 0/1). Continuous variables allow the model to predict in relation to a quantitative scale, whereas indicator variables predict changes based on categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c98297",
   "metadata": {},
   "source": [
    "3.Effect of Adding an Indicator Variable with a Continuous Variable in MLR: Introducing an indicator variable alongside a continuous variable in MLR enables the model to make predictions for different groups. The model’s behavior changes because it can now account for group differences, effectively fitting parallel lines for each group based on the continuous predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b607a",
   "metadata": {},
   "source": [
    "4.Effect of Adding Interaction between Continuous and Indicator Variables in MLR: Adding an interaction term allows the effect of the continuous variable to vary across groups (indicator variable). This interaction causes the regression lines for each group to have different slopes, indicating that the continuous predictor’s impact depends on the group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7ce18",
   "metadata": {},
   "source": [
    "5.Behavior of an MLR Model Based Only on Indicator Variables from a Non-Binary Categorical Variable: When using only indicator variables from a categorical variable with multiple categories, the model predicts outcomes based solely on category membership. This requires creating binary (dummy) variables for each category, representing the presence of each category. This approach models different baseline levels for each category, making it effective for purely categorical predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8911223",
   "metadata": {},
   "source": [
    "question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c3663d",
   "metadata": {},
   "source": [
    "1.Identifying Outcome and Predictor Variables: The outcome variable here would likely be the sales of sports equipment, as this is the measurable result the company wants to influence. The predictor variables would be the amounts spent on TV advertising and online advertising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e43744",
   "metadata": {},
   "source": [
    "2.Meaningful Interactions:A meaningful interaction might exist between TV and online advertising spending, as their  combined effect on sales could be different from the sum of their individual effects. For example, a high budget on TV ads might be more effective if paired with a high budget for online ads, suggesting that an interaction term between TV and online ad spend might improve predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c81b4b",
   "metadata": {},
   "source": [
    "3.Linear Forms Without and With Interaction:\n",
    "Without Interaction:sales=β0+β1(TV spend)+β2(Online spend)+ϵ; Here, the model assumes that TV and online ad spending independently affect sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea5eaf4",
   "metadata": {},
   "source": [
    "With Interaction: sales=β0+β1(TV spend)+β2(Online spend)+β3(TV spend×Online spend)+ϵ\n",
    "The coefficient β3 represents how one type of ad spending influences the effectiveness of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa523a59",
   "metadata": {},
   "source": [
    "4.Predictions with and without Interaction: Without Interaction: Predictions are straightforward sums of the TV and online ad effects. Each dollar increase in TV spend or online spend leads to a fixed increase in sales, independently of the other. With Interaction: Predictions incorporate a term that adjusts sales based on the combined levels of both TV and online ad spending. This model might predict higher sales when both ad spends are high, capturing the synergy between the two advertising channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e804801",
   "metadata": {},
   "source": [
    "5.Updating the Models for Binary (High/Low) Ad Budgets: If TV and online ad budgets are categorized as “high” or “low” (binary), we treat each as an indicator variable. For example, let TV be 1 if high budget and 0 if low, and Online as 1 for high budget and 0 for low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ac44c",
   "metadata": {},
   "source": [
    "Without Interaction:sales=β0+β1(TV)+β2(Online)+ϵ. Here, β1and β2 reflect the change in sales associated with a high budget (vs. low) for each ad type independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0bf1eb",
   "metadata": {},
   "source": [
    "With Interaction: sales=β0+β1(TV)+β2(Online)+β3(TV×Online)ϵ. The interaction term (TV×Online) accounts for cases where both advertising budgets are high, allowing the model to capture any synergistic effect when both are maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f190360",
   "metadata": {},
   "source": [
    "question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d7eaed",
   "metadata": {},
   "source": [
    "without interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "234e05e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  has_friends  connected\n",
      "0    56            1          0\n",
      "1    69            1          0\n",
      "2    46            0          0\n",
      "3    32            0          0\n",
      "4    60            1          0\n",
      "5    22            0          1\n",
      "6    27            1          1\n",
      "7    49            1          1\n",
      "8    31            1          1\n",
      "9    64            0          0\n",
      "10   42            1          1\n",
      "11   34            1          1\n",
      "12   29            1          1\n",
      "13   39            0          0\n",
      "14   70            1          0\n",
      "15   35            0          0\n",
      "16   36            1          1\n",
      "17   55            0          0\n",
      "18   52            1          1\n",
      "19   26            1          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'age': [56, 69, 46, 32, 60, 22, 27, 49, 31, 64, 42, 34, 29, 39, 70, 35, 36, 55, 52, 26],\n",
    "    'has_friends': [1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1],\n",
    "    'connected': [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1]\n",
    "}\n",
    "\n",
    "sample_data = pd.DataFrame(data)\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08156ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: inf\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:2383: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:2441: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(self.cdf(q * linpred)))\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformula\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msmf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m formula_additive \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnected ~ age + has_friends\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m log_reg_model_additive \u001b[38;5;241m=\u001b[39m \u001b[43msmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformula_additive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m log_reg_model_additive\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:2599\u001b[0m, in \u001b[0;36mLogit.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m   2596\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(DiscreteModel\u001b[38;5;241m.\u001b[39mfit\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m)\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, start_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m, maxiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m35\u001b[39m,\n\u001b[1;32m   2598\u001b[0m         full_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, disp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2599\u001b[0m     bnryfit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2607\u001b[0m     discretefit \u001b[38;5;241m=\u001b[39m LogitResults(\u001b[38;5;28mself\u001b[39m, bnryfit)\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BinaryResultsWrapper(discretefit)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:243\u001b[0m, in \u001b[0;36mDiscreteModel.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# TODO: make a function factory to have multiple call-backs\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m mlefit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mlefit\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/statsmodels/base/model.py:582\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m     Hinv \u001b[38;5;241m=\u001b[39m cov_params_func(\u001b[38;5;28mself\u001b[39m, xopt, retvals)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m full_output:\n\u001b[0;32m--> 582\u001b[0m     Hinv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mretvals\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHessian\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m nobs\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_hessian:\n\u001b[1;32m    584\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhessian(xopt)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/numpy/linalg/linalg.py:538\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    536\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    537\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 538\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/numpy/linalg/linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "formula_additive = 'connected ~ age + has_friends'\n",
    "log_reg_model_additive = smf.logit(formula_additive, data=sample_data).fit()\n",
    "log_reg_model_additive.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "formula_interaction = 'connected ~ age * has_friends'\n",
    "log_reg_model_interaction = smf.logit(formula_interaction, data=sample_data).fit(maxiter=100)\n",
    "log_reg_model_interaction.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72926eb6",
   "metadata": {},
   "source": [
    "question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc52f6",
   "metadata": {},
   "source": [
    "1.Low Variability Explained (R-squared): The R-squared value, which is 17.6% in this case, represents the proportion of the variance in the dependent variable HP explained by the independent variables. An R-squared of 17.6% means that most of the variation in HP is due to factors not included in the model. Thus, the model captures only a limited portion of the factors affecting HP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94fcee",
   "metadata": {},
   "source": [
    "2.Significant Coefficients: Coefficient size and significance measure different aspects than variance explained. Large coefficients indicate a strong effect of the predictors on HP relative to each other, while significance levels (p-values) test whether these effects are statistically different from zero. Strong evidence against the null hypothesis of \"no effect\" indicates that each predictor (e.g., Sp. Def or Generation) reliably contributes to changes in HP when tested in isolation, even though their combined effect does not account for much of HP's total variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908a403",
   "metadata": {},
   "source": [
    "3.Why They Can Coexist: A model can have large, statistically significant coefficients if it identifies strong effects for individual predictors within a relatively noisy system. Here, even though the model identifies reliable predictor effects, the unexplained variability (the remaining 82.4%) suggests that there are likely additional unmeasured factors or inherent randomness in HP that the model doesn’t account for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead25866",
   "metadata": {},
   "source": [
    "These results indicate that while Sp. Def, Generation, and their interaction meaningfully contribute to HP, there are many other unexplained factors driving HP. This is a common finding in complex systems where only part of the variability can be attributed to the predictors in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece8ad6",
   "metadata": {},
   "source": [
    "question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8295ac",
   "metadata": {},
   "source": [
    "1.This cell prepares the Pokémon data by replacing any missing values in the \"Type 2\" column with \"None\". It then creates a 50-50 train-test split. The train_test_split function, with a 50-50 split, ensures that we have equal datasets for training and testing, which will be used to evaluate the model's performance both within (in-sample) and outside (out-of-sample) the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30faf32c",
   "metadata": {},
   "source": [
    "2.This code fits a simple linear regression model on the training data with HP as the outcome variable and Attack and Defense as predictors. This model serves as a basic benchmark, allowing us to see how well Attack and Defense predict HP. The model summary provides insight into the coefficients, their significance, and the in-sample R-squared for model 3, which quantifies the percentage of variation in HP that Attack and Defense can explain within the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94632829",
   "metadata": {},
   "source": [
    "3.This cell calculates both the in-sample and out-of-sample R-squared values for model 3. The in-sample R-squared shows how well the model explains HP variability on the training data. The out-of-sample R-squared checks model 3's performance on the test data (unseen during training). Comparing these two values reveals any potential overfitting. If the in-sample R-squared is significantly higher than the out-of-sample R-squared, it suggests that model 3 may not generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafdc894",
   "metadata": {},
   "source": [
    "4.This cell builds a much more complex model (model 4) that includes multiple predictors (e.g., Speed, Legendary, Sp. Def, Sp. Atk) and allows for higher-order interaction terms. This model aims to capture intricate relationships between HP and various predictors by including all possible interactions. The warning about adding Generation and Type interactions (6 * 18 * 19 combinations) reminds us of the risk of exponential growth in interactions, which could make the model computationally infeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be6669",
   "metadata": {},
   "source": [
    "5.This cell calculates the in-sample and out-of-sample R-squared values for thIf model 4’s out-of-sample R-squared is significantly lower than its in-sample R-squared, it would indicate overfitting. In contrast, if both values are similar and high, model 4 generalizes well.e complex model (model 4). Model 4 is expected to have a much higher in-sample R-squared than model 3 because it captures more of the variability within the training data by accounting for multiple predictors and interactions. Comparing model 3 and model 4’s out-of-sample R-squared values helps decide whether adding complexity (interactions) yields better predictive performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d595b",
   "metadata": {},
   "source": [
    "Summary of What These Cells Illustrate\n",
    "These five cells illustrate model specification, training vs. testing performance, and overfitting:\n",
    "Model 3 (simple model) serves as a baseline to see if basic predictors (Attack and Defense) can predict HP well.\n",
    "Model 4 (complex model with interactions) aims to capture more relationships, illustrating the risk of overfitting.\n",
    "In-sample vs. Out-of-sample R-squared comparisons allow us to check if model 4’s added complexity improves prediction or merely memorizes training data.\n",
    "This workflow emphasizes the trade-off between model simplicity and interpretability versus model complexity and predictive power. It highlights the importance of balancing complexity to avoid overfitting and achieve generalizable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996269c",
   "metadata": {},
   "source": [
    "question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2293f42",
   "metadata": {},
   "source": [
    "1.Model 4 Design Matrix and Multicollinearity: When you define model4_linear_form, specifying many interactions (e.g., Attack * Defense * Speed * Legendary * Sp. Def * Sp. Atk), this form generates multiple new predictor columns in the design matrix model4_spec.exog.\n",
    "Each interaction term (e.g., Attack * Defense, Attack * Defense * Speed) creates new predictor variables representing combined effects, leading to a large and complex design matrix.\n",
    "Multicollinearity arises because many of these new predictors are mathematically related, meaning they contain redundant information (i.e., high correlation between columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439cb375",
   "metadata": {},
   "source": [
    "2.Effect of Multicollinearity on Out-of-Sample Predictions:\n",
    "High multicollinearity causes the model to fit very closely to the training data, effectively \"memorizing\" it. This lack of independent information across predictors prevents the model from generalizing well to new (out-of-sample) data.\n",
    "The high multicollinearity in model4_spec.exog leads to unstable coefficient estimates and, ultimately, poor predictive performance on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1f219",
   "metadata": {},
   "source": [
    "3.Condition Number as an Indicator of Multicollinearity:\n",
    "The condition number in regression analysis indicates the degree of multicollinearity. A high condition number suggests severe multicollinearity, making the model unstable.\n",
    "For model4_fit, the condition number is extremely high (around 10 ** 16), signaling excessive multicollinearity. Even after centering and scaling predictors (e.g., scale(center(Attack))), the condition number remains very high, as multicollinearity remains among interaction terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66485b98",
   "metadata": {},
   "source": [
    "4.Centering and Scaling:\n",
    "Centering (subtracting the mean) and scaling (dividing by standard deviation) reduce multicollinearity slightly by removing correlations between raw predictors. This is why model3_center_scale_fit has a vastly improved condition number (from 343.0 down to 1.66).\n",
    "However, in complex models like model4_CS_fit, where interaction terms are numerous, centering and scaling cannot fully mitigate multicollinearity from the high-dimensional interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229e99fa",
   "metadata": {},
   "source": [
    "5.Multicollinearity in complex models like model4 creates redundancy in the design matrix, leading to unstable coefficients and poor out-of-sample performance. Although centering and scaling reduce multicollinearity, they can’t resolve it entirely in high-interaction models, as indicated by the persistently high condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c759e7",
   "metadata": {},
   "source": [
    "question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755faffe",
   "metadata": {},
   "source": [
    "Model 5: Expanding and Simplifying.\n",
    "Rationale: Adds more predictors (Attack, Defense, Speed, Legendary, Sp. Def, Sp. Atk) without excessive interactions.\n",
    "Purpose: Capture more variability while avoiding overfitting.\n",
    "Result: Improved in-sample explanatory power and generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a1416d",
   "metadata": {},
   "source": [
    "Model 6: Refining with Statistical Significance\n",
    "Rationale: Selects only significant predictors from Model 5.\n",
    "Purpose: Reduce multicollinearity and enhance interpretability.\n",
    "Result: Simplified model with improved out-of-sample R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56641e",
   "metadata": {},
   "source": [
    "Model 7: Adding Interactions and Scaling. \n",
    "Rationale: Reintroduces interactions for key predictors and applies centering and scaling.\n",
    "Purpose: Capture synergistic effects and reduce multicollinearity.\n",
    "Result: Better numerical stability and balanced performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da77855",
   "metadata": {},
   "source": [
    "Each successive model (5, 6, and 7) incrementally builds on the previous one by selecting important predictors, reducing unnecessary complexity, and addressing multicollinearity through selective interactions and scaling. This progressive approach results in a model (model 7) that balances interpretability, predictive power, and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdeb611",
   "metadata": {},
   "source": [
    "question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96751a33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pokeaman' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m out_of_sample_Rsquared \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(reps)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(reps):\n\u001b[0;32m---> 15\u001b[0m     pokeaman_train, pokeaman_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mpokeaman\u001b[49m, train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     18\u001b[0m     model_fit \u001b[38;5;241m=\u001b[39m smf\u001b[38;5;241m.\u001b[39mols(formula\u001b[38;5;241m=\u001b[39mlinear_form, data\u001b[38;5;241m=\u001b[39mpokeaman_train)\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m     20\u001b[0m     in_sample_Rsquared[i] \u001b[38;5;241m=\u001b[39m model_fit\u001b[38;5;241m.\u001b[39mrsquared\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pokeaman' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "linear_form = 'HP ~ Attack + Defense'\n",
    "\n",
    "reps = 100\n",
    "in_sample_Rsquared = np.zeros(reps)\n",
    "out_of_sample_Rsquared = np.zeros(reps)\n",
    "\n",
    "for i in range(reps):\n",
    "    pokeaman_train, pokeaman_test = train_test_split(pokeaman, train_size=0.5)\n",
    "    \n",
    "\n",
    "    model_fit = smf.ols(formula=linear_form, data=pokeaman_train).fit()\n",
    "\n",
    "    in_sample_Rsquared[i] = model_fit.rsquared\n",
    "    \n",
    "\n",
    "    y_test = pokeaman_test.HP\n",
    "    y_pred_test = model_fit.predict(pokeaman_test)\n",
    "    out_of_sample_Rsquared[i] = np.corrcoef(y_test, y_pred_test)[0, 1] ** 2\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    \"In Sample Performance (R-squared)\": in_sample_Rsquared,\n",
    "    \"Out of Sample Performance (R-squared)\": out_of_sample_Rsquared\n",
    "})\n",
    "\n",
    "fig = px.scatter(df_results, \n",
    "                 x=\"In Sample Performance (R-squared)\", \n",
    "                 y=\"Out of Sample Performance (R-squared)\",\n",
    "                 title=\"In-Sample vs Out-of-Sample R-squared (Repeated Splits)\")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], \n",
    "                         mode=\"lines\", \n",
    "                         name=\"y=x\", \n",
    "                         line=dict(dash=\"dash\")))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf075b",
   "metadata": {},
   "source": [
    "1.Meaning of results: Evaluating model performance through multiple train-test splits helps assess generalizability, stability, and overfitting. Consistent out-of-sample R-squared values close to in-sample values indicate a robust model that generalizes well to new data. Stable performance across different splits shows the model captures underlying patterns rather than specific data idiosyncrasies. Significant differences between in-sample and out-of-sample R-squared values suggest overfitting, indicating the need for model adjustments. This approach ensures the model is reliable and effective for future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e7eb64",
   "metadata": {},
   "source": [
    "2.Purpose of the Demonstration: Generalizability and Stability: Checking if the model performs consistently across different data subsets. Overfitting Detection: Identifying if the model performs well on training data but poorly on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29496780",
   "metadata": {},
   "source": [
    "question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4edd3bd",
   "metadata": {},
   "source": [
    "This demonstration underscores the importance of generalizability across generations, illustrating that models trained on older data (e.g., Generation 1 Pokémon) may struggle to generalize to new data due to evolving attributes and relationships. It highlights that training on a broader set of generations (Generations 1-5) typically results in better out-of-sample performance on Generation 6 compared to training on a single generation. This suggests that a diverse training set helps create a more robust model capable of handling new data variations. Additionally, the demonstration compares the impact of model complexity, showing that while a more complex model (Model 7) might fit a specific generation well, it may not generalize as effectively as a simpler model (Model 6) on future data. This comparison emphasizes the trade-off between model complexity and generalizability, advocating for simpler models when aiming for robust performance across different data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f079ea",
   "metadata": {},
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b938b",
   "metadata": {},
   "source": [
    "In this conversation, we explored the progression of models in terms of added complexity, overfitting, and generalizability. Starting with models 5, 6, and 7, we observed how each iteration added predictors and interactions, gradually increasing complexity. This approach often improves in-sample performance (R-squared on training data) but risks overfitting, which can reduce the model’s ability to generalize to new data (out-of-sample R-squared). We then used a for loop to repeatedly split the data into train and test sets, assessing the consistency of in-sample and out-of-sample R-squared values. This demonstrated that stable performance across splits indicates a model’s reliability on new data. Next, we trained models on limited subsets of Pokémon generations (e.g., Generation 1) and evaluated their performance on future generations, finding that models trained on broader data (e.g., Generations 1-5) generalized better to unseen data (Generation 6). This highlighted the importance of training data diversity in improving prediction accuracy for new scenarios. Additionally, we addressed the issue of multicollinearity, as seen in model 7’s high condition number, which impacts stability and out-of-sample performance. Centering, scaling, and selective interactions helped reduce multicollinearity, balancing model complexity with generalizability. Overall, the conversation underscored the need to balance data diversity, complexity, and multicollinearity to develop models that perform robustly across both current and future data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc371fe",
   "metadata": {},
   "source": [
    "chatbox link: https://chatgpt.com/share/673672d5-dffc-800c-8139-673fda3cf8cd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
